{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.2 - Clustering Load Profiles\n",
    "\n",
    "Load profiling is a crucial aspect of energy consumption analysis that involves collecting and analyzing data on the energy usage of a system or building. Clustering, on the other hand, is a machine learning technique used to group similar data points together.\n",
    "\n",
    "In this exercise session, we will learn how to use Python to cluster load profiles. We will start by exploring what load profiles are, how they are collected and why they are important. Next, we will delve into the theory behind clustering and its applications in load profiling. We will then work through a step-by-step guide to using Python's scikit-learn library to perform load profile clustering.\n",
    "\n",
    "By the end of this exercise session, you will have a good understanding of load profiles, clustering techniques and how to apply these techniques in Python to perform load profile clustering. You will also be able to interpret the results of the clustering and use them to make informed decisions about energy consumption management. So, let's get started!\n",
    "\n",
    "### Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import calmap\n",
    "from matplotlib.colors import ListedColormap, Normalize\n",
    "\n",
    "df = pd.read_csv(\"data/LoadTimeSeriesData_case5.csv\", parse_dates=['timestamp'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Employing what you learned in the previous lab use one of the previous nas replacement methods to fill nas if present within the *power* variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['power'].interpolate(method='spline', inplace=True, order=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform load profiles clustering, we need to re-organize our dataset into a MxN matrix where:\n",
    "* M is the number of days in our dataset\n",
    "* N is the frequency of our timeseries (i.e. hour)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Assign new columns for data and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['timestamp'].dt.date\n",
    "df['hour'] = df['timestamp'].dt.hour"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate the MxN matrix using the new columns. The *date* should be used as index and *hour* as columns.\n",
    "\n",
    "*hint*: use the `pivot` method of pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matrix = df.pivot(index='date', columns='hour', values='power')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Task**: perfrom clustering using KMeans method on `df_matrix`. Select a value for K (desired number of clusters). Then extract the cluster labels from the results obtained and assign them to a new object called `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "K = 5\n",
    "kmeans = KMeans(n_clusters=K, random_state=0).fit(df_matrix)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Task**: count how many elements were assigned to each cluster from the object `labels` just created.\n",
    "\n",
    "What are your toughts about the clusters identifed? Are the number of elements balanced between each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = np.unique(labels, return_counts=True)\n",
    "cluster_counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Index and Davies Bouldin Index for Cluster Assessment\n",
    "\n",
    "When it comes to clustering algorithms, it's important to assess their quality and effectiveness. Two popular measures for evaluating the performance of clustering algorithms are the Silhouette Index and the Davies Bouldin Index.\n",
    "\n",
    "In scikit-learn, the Silhouette Index can be calculated using the silhouette_score function, while the Davies Bouldin Index can be calculated using the davies_bouldin_score function.\n",
    "\n",
    "The Silhouette Index is a measure of how well each data point fits into its assigned cluster, based on both the distance to the points in the same cluster and the distance to the points in the nearest neighboring cluster. The score ranges from -1 to 1, with a higher score indicating better clustering.\n",
    "\n",
    "The Davies Bouldin Index, on the other hand, measures the average similarity between each cluster and its most similar cluster, taking into account the distance between their centroids. A lower score indicates better clustering.\n",
    "\n",
    "Both measures can be useful in determining the optimal number of clusters for a given dataset, as well as comparing the performance of different clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "sh = silhouette_score(df_matrix, kmeans.labels_)\n",
    "db = davies_bouldin_score(df_matrix, kmeans.labels_)\n",
    "\n",
    "print(f\"silhouette score for {K} clusters: {sh}\")\n",
    "print(f\"db index for {K} clusters: {db}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Try to determine the best number of clusters between 2 and 8 according to Silhouette and Davies Bouldin. Print the results obtained.\n",
    "\n",
    "- The two metrics return the same value? \n",
    "- How can we effectively select the number of K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_range = [2, 3, 4, 5, 6, 7, 8]\n",
    "db_list = []\n",
    "sh_list = []\n",
    "\n",
    "for K in K_range:\n",
    "    kmeans = KMeans(n_clusters=K, random_state=0).fit(df_matrix)\n",
    "    sh_list.append(silhouette_score(df_matrix, kmeans.labels_)) \n",
    "    db_list.append(davies_bouldin_score(df_matrix, kmeans.labels_))\n",
    "\n",
    "print(f\"The best number of clusters according to Silhouette is {K_range[np.argmax(sh_list)]}\")\n",
    "print(f\"The best number of clusters according to DB Index is {K_range[np.argmin(db_list)]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroids\n",
    "\n",
    "In the context of load profiles clustering, centroids are usually represented as the average load profile of the cluster. Analyzing the centroids and their relationship with the elements of their relative cluster can provide useful hints about the goodness of a clustering process.\n",
    "\n",
    "7. Evaluate the average profile (i.e. evaluate the mean of power for each cluster and each hour). Perfom again the kmeans clustering process by implementing the best number of cluster according to one of the previous metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "K = 4\n",
    "kmeans = KMeans(n_clusters=K, random_state=0).fit(df_matrix)\n",
    "labels = kmeans.labels_\n",
    "cluster_counts = np.unique(labels, return_counts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  add the cluster label to the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster labels to the original dataset\n",
    "df['cluster'] = np.repeat(labels, 24)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- calculate the centroids. *Hint*: use the groupby() function of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new DataFrame with the average power for each hour of the day and for each cluster\n",
    "centroids = df.groupby(['cluster', 'hour'])['power'].mean().reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - plot the load profiles for each cluster and the centroid. *Hint*: employ the code of the previous lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating load profiles\n",
    "g = sns.FacetGrid(data=df, col='cluster', hue='date', col_wrap=3, height=3, aspect=2, sharey=False)\n",
    "g.map(sns.lineplot, 'hour', 'power', color='gray')\n",
    "\n",
    "# adding average values\n",
    "i = 0\n",
    "for ax, cluster in zip(g.axes.flatten(), centroids['cluster'].unique()):\n",
    "    sns.lineplot(x='hour', y='power', data=centroids[centroids['cluster'] == cluster], color='r', ax=ax, label='Profilo medio', legend=False)\n",
    "    ax.set_ylim(bottom=0, top=df['power'].max())\n",
    "    ax.set_xticks(range(0, 24))\n",
    "    ax.grid(True, linestyle='--')\n",
    "\n",
    "    cluster_counts_str = 'Count: ' + str(cluster_counts[1][i])\n",
    "    ax.text(0.05, 0.95, cluster_counts_str, transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "    i += 1\n",
    "    \n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about the clustering obtained. \n",
    "\n",
    "- Are the centroids representative of the daily load profiles within their clusters?\n",
    "- Do you think it would be interesting to observe the results changing the value of *K*?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe how clusters of load profiles are distributed across the year.\n",
    "\n",
    "To this purpose a useful visualization is the calendar plot. Run the following code and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_data = pd.DataFrame({'cluster': labels}, index=pd.to_datetime(df_matrix.index))\n",
    "\n",
    "# define colors accordin to palette\n",
    "colors = sns.color_palette(\"husl\", K)\n",
    "hex_colors = ['#{:02x}{:02x}{:02x}'.format(int(color[0]*255), int(color[1]*255), int(color[2]*255)) for color in colors]\n",
    "cmap = ListedColormap(hex_colors)\n",
    "\n",
    "fig, ax = calmap.calendarplot(cal_data['cluster'], cmap=cmap, \n",
    "                              fillcolor='grey', linewidth=0.5, fig_kws=dict(figsize=(12, 4)), monthticks=3, daylabels='MTWTFSS')\n",
    "\n",
    "# add a colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=Normalize(vmin=0, vmax=K-1))\n",
    "sm.set_array([])\n",
    "cax = fig.add_axes([0.3, 0.9, 0.4, 0.05])\n",
    "cb = plt.colorbar(sm, cax=cax, orientation='horizontal')\n",
    "cb.set_label('Cluster')\n",
    "cb.set_ticks(np.sort(cal_data['cluster'].unique()))\n",
    "cb.set_ticklabels(np.sort(cal_data['cluster'].unique()))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: try out different values of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "K = 6\n",
    "kmeans = KMeans(n_clusters=K, random_state=0).fit(df_matrix)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "cluster_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "# Add the cluster labels to the original dataset\n",
    "df['cluster'] = np.repeat(labels, 24)\n",
    "\n",
    "# creating a new DataFrame with the average power for each hour of the day and for each cluster\n",
    "centroids = df.groupby(['cluster', 'hour'])['power'].mean().reset_index()\n",
    "\n",
    "# generating load profiles\n",
    "g = sns.FacetGrid(data=df, col='cluster', hue='date', col_wrap=3, height=3, aspect=2, sharey=False)\n",
    "g.map(sns.lineplot, 'hour', 'power', color='gray')\n",
    "\n",
    "# adding average values\n",
    "i = 0\n",
    "for ax, cluster in zip(g.axes.flatten(), centroids['cluster'].unique()):\n",
    "    sns.lineplot(x='hour', y='power', data=centroids[centroids['cluster'] == cluster], color='r', ax=ax, label='Profilo medio', legend=False)\n",
    "    ax.set_ylim(bottom=0, top=df['power'].max())\n",
    "    ax.set_xticks(range(0, 24))\n",
    "    ax.grid(True, linestyle='--')\n",
    "\n",
    "    cluster_counts_str = 'Count: ' + str(cluster_counts[1][i])\n",
    "    ax.text(0.05, 0.95, cluster_counts_str, transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "    i += 1\n",
    "\n",
    "plt.show()\n",
    "\n",
    "cal_data = pd.DataFrame({'cluster': labels}, index=pd.to_datetime(df_matrix.index))\n",
    "\n",
    "# define colors accordin to palette\n",
    "colors = sns.color_palette(\"husl\", K)\n",
    "hex_colors = ['#{:02x}{:02x}{:02x}'.format(int(color[0]*255), int(color[1]*255), int(color[2]*255)) for color in colors]\n",
    "cmap = ListedColormap(hex_colors)\n",
    "\n",
    "fig, ax = calmap.calendarplot(cal_data['cluster'], cmap=cmap, \n",
    "                              fillcolor='grey', linewidth=0.5, fig_kws=dict(figsize=(12, 4)), monthticks=3, daylabels='MTWTFSS')\n",
    "\n",
    "# add a colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=Normalize(vmin=0, vmax=K-1))\n",
    "sm.set_array([])\n",
    "cax = fig.add_axes([0.3, 0.9, 0.4, 0.05])\n",
    "cb = plt.colorbar(sm, cax=cax, orientation='horizontal')\n",
    "cb.set_label('Cluster')\n",
    "cb.set_ticks(cal_data['cluster'].unique())\n",
    "cb.set_ticklabels(cal_data['cluster'].unique())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Hierarchical clustering\n",
    "\n",
    "Agglomerative clustering is a popular hierarchical clustering method used to group similar data points into clusters. It works by initially treating each data point as its own cluster and then iteratively merging the closest clusters until only one cluster containing all data points remains. The merging process is guided by a linkage method that determines how the distance between two clusters is measured.\n",
    "\n",
    "There are several linkage methods used in agglomerative clustering, each with its own advantages and disadvantages.\n",
    "\n",
    "The **single** linkage method, also known as the nearest-neighbor method, calculates the distance between the closest two data points in each cluster and merges the two clusters with the smallest distance. This method tends to produce long, narrow clusters and can be sensitive to noise and outliers.\n",
    "\n",
    "The **complete** linkage method, also known as the farthest-neighbor method, calculates the distance between the furthest two data points in each cluster and merges the two clusters with the largest distance. This method tends to produce compact, spherical clusters but can also be sensitive to noise and outliers.\n",
    "\n",
    "The **average** linkage method calculates the average distance between all pairs of data points in each cluster and merges the two clusters with the smallest average distance. This method can be less sensitive to noise and outliers than the single and complete linkage methods and tends to produce more balanced clusters.\n",
    "\n",
    "The **Ward** linkage method minimizes the increase in variance of the clusters resulting from merging them. It tends to produce compact, spherical clusters of similar size and is less sensitive to noise and outliers than the other linkage methods.\n",
    "\n",
    "Overall, the choice of linkage method in agglomerative clustering depends on the nature of the data and the goals of the analysis. Each method has its own strengths and weaknesses, and selecting the appropriate method can lead to more meaningful and accurate clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None, linkage=\"complete\").fit(df_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "# plot the top (p) levels of the dendrogram\n",
    "plot_dendrogram(model, truncate_mode=\"level\", p=0)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Analyze different *linkage methods* and *number of K* to understand the differences between the different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "K = 8\n",
    "linkage_method = \"single\" # \"complete\", \"single\", \"average\", \"ward\"\n",
    "model = AgglomerativeClustering(distance_threshold=None, n_clusters=K, linkage=linkage_method).fit(df_matrix)\n",
    "labels = model.labels_\n",
    "\n",
    "cluster_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "# Add the cluster labels to the original dataset\n",
    "df['cluster'] = np.repeat(labels, 24)\n",
    "\n",
    "# creating a new DataFrame with the average power for each hour of the day and for each cluster\n",
    "centroids = df.groupby(['cluster', 'hour'])['power'].mean().reset_index()\n",
    "\n",
    "# generating load profiles\n",
    "g = sns.FacetGrid(data=df, col='cluster', hue='date', col_wrap=3, height=3, aspect=2, sharey=False)\n",
    "g.map(sns.lineplot, 'hour', 'power', color='gray')\n",
    "\n",
    "# adding average values\n",
    "i = 0\n",
    "for ax, cluster in zip(g.axes.flatten(), centroids['cluster'].unique()):\n",
    "    sns.lineplot(x='hour', y='power', data=centroids[centroids['cluster'] == cluster], color='r', ax=ax, label='Profilo medio', legend=False)\n",
    "    ax.set_ylim(bottom=0, top=df['power'].max())\n",
    "    ax.set_xticks(range(0, 24))\n",
    "    ax.grid(True, linestyle='--')\n",
    "    \n",
    "    cluster_counts_str = 'Count: ' + str(cluster_counts[1][i])\n",
    "    ax.text(0.05, 0.95, cluster_counts_str, transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "    i += 1\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
